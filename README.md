
# ðŸ“¸ Image Captioning App

## Overview
A full-stack web application that generates descriptive captions for uploaded images.  

- **Backend**: Flask-based API using BLIP model for captioning  
- **Frontend**: Next.js (React) interface for uploading images & displaying results  

---

## ðŸš€ Table of Contents

- [Features](#features)  
- [Tech Stack](#tech-stack)  
- [Getting Started](#getting-started)  
  - [Prerequisites](#prerequisites)  
  - [Backend Setup](#backend-setup)  
  - [Frontend Setup](#frontend-setup)  
- [Usage](#usage)  
- [Project Structure](#project-structure)  
- [Model & Inference](#model--inference)  
- [Screenshots / Demo](#screenshots--demo)  
- [License](#license)  

---

## âœ¨ Features

- Upload images via a web interface
- Flask API processes the image using BLIP from HuggingFace
- Model generates natural-language captions
- Captions shown instantly in the Next.js frontend

---

## ðŸ§° Tech Stack

| Layer      | Technology                          |
|------------|-------------------------------------|
| Frontend   | Next.js (React)                     |
| Backend    | Flask                               |
| ML Model   | Salesforce BLIP (BLIP-base)         |
| API Comm.  | RESTful (Axios / fetch)             |
| CORS       | Flask-CORS                          |

---

## ðŸ Getting Started

### Prerequisites

- Node.js & npm or yarn  
- Python 3.8+  
- Virtual environment (venv, conda, etc.)  

---

### Backend Setup

```bash
cd backend/
python -m venv venv
source venv/bin/activate     # or .\venv\Scripts\activate on Windows
pip install -r requirements.txt
```

Your `requirements.txt` should contain:

```
flask
flask-cors
transformers
torch
pillow
```

Run the Flask API:

```bash
python app.py
```

Default address: `http://localhost:5000`

---

### Frontend Setup

```bash
cd frontend/
npm install
npm run dev      # or yarn dev
```

Frontend should be accessible at: `http://localhost:3000`

---

## ðŸ§ª Usage

1. Open the frontend in your browser  
2. Choose an image file and upload it  
3. The backend generates a caption via the BLIP model  
4. Caption is displayed next to your image  

---

## ðŸ“ Project Structure

### Backend

```
backend/
â”œâ”€â”€ app.py                # Flask API using BLIP
â”œâ”€â”€ requirements.txt
```

### Frontend

```

â”œâ”€â”€ app/                    # App directory (Next.js 13+)
â”‚   â”œâ”€â”€ layout.tsx         # Global layout wrapper
â”‚   â”œâ”€â”€ page.tsx           # Main UI page (image upload & result)
â”‚   â””â”€â”€ favicon.ico        # Favicon
â”‚
â”œâ”€â”€ components/            # Shared and reusable UI components
â”‚   â””â”€â”€ ui/                # Shadcn-like UI primitives
â”‚       â”œâ”€â”€ alert.tsx
â”‚       â”œâ”€â”€ button.tsx
â”‚       â””â”€â”€ card.tsx
â”‚
â”œâ”€â”€ lib/                   # Utility functions
â”‚   â””â”€â”€ utils.ts
â”‚
â”œâ”€â”€ public/                # Public static files
â”‚   â””â”€â”€ screenshots/       # (Optional) screenshots to display in README
â”‚
â”œâ”€â”€ styles/ or globals     # Global CSS
â”‚   â””â”€â”€ globals.css
â”‚
â”œâ”€â”€ next.config.ts         # Next.js config
â”œâ”€â”€ tsconfig.json          # TypeScript config
â”œâ”€â”€ package.json           # Dependencies & scripts
â”œâ”€â”€ postcss.config.mjs     # PostCSS configuration
```

---

## ðŸ§  Model & Inference

- The backend uses the `Salesforce/blip-image-captioning-base` model via HuggingFace Transformers.
- An image is uploaded to the Flask endpoint `/caption`.
- The model processes the image and returns a generated natural language caption.
- The model uses GPU (if available), otherwise falls back to CPU.

Example API usage:
```bash
curl -X POST -F "image=@your_image.jpg" http://localhost:5000/caption
```

Sample JSON response:
```json
{
  "caption": "a cat sitting on a wooden table"
}
```

---

## ðŸ“¸ Screenshots / Demo

> Add screenshots in `frontend/public/screenshots` and link them here:

```md
![Upload Page](./frontend/public/screenshots/upload.png)
![Caption Output](./frontend/public/screenshots/result.png)
```

---


## ðŸ“„ License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---

